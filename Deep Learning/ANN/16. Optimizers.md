![image](https://github.com/user-attachments/assets/739a34f6-8c8f-46b6-b0db-68d1cdb7bc1f)

# Optimizers in Deep Learning

In deep learning, optimizers are algorithms that adjust the weights of a neural network to minimize the loss function during training. Here, we explore some of the most commonly used optimizers: Stochastic Gradient Descent (SGD), SGD with Momentum, Adagrad, RMSprop, and Adam. We will discuss their mathematical formulations, intuition, advantages, disadvantages, and scenarios for use.

## 1. Stochastic Gradient Descent (SGD)

- **Math**:  
  $$ 
  \theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta) 
  $$

- **Intuition**: 
  - Updates parameters using the gradient of the loss function with respect to the weights. It makes noisy progress toward the global minimum.

- **Advantages**: 
  - Simple to implement and useful for large datasets.

- **Disadvantages**: 
  - Noisy updates can lead to slower convergence and getting stuck in local minima.

- **When to Use**: 
  - Suitable for convex or simpler loss functions.

- **When Not to Use**: 
  - Avoid for non-convex functions or deep networks.

---

## 2. SGD with Momentum

- **Math**:  
  $$ 
  v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta J(\theta) 
  $$
  $$ 
  \theta_{t+1} = \theta_t - \eta v_t 
  $$

- **Intuition**: 
  - Accumulates gradients to build velocity, helping escape local minima.

- **Advantages**: 
  - Accelerates convergence and helps avoid local minima.

- **Disadvantages**: 
  - May cause overshooting and oscillation around the optimal point.

- **When to Use**: 
  - Deep networks and complex loss landscapes.

- **When Not to Use**: 
  - Not recommended for noisy, erratic gradients.

---

## 3. Adagrad

- **Math**:  
  $$ 
  \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta J(\theta) 
  $$

- **Intuition**: 
  - Adapts the learning rate based on past gradients, useful for sparse data problems.

- **Advantages**: 
  - Automatic learning rate adjustment for each parameter.

- **Disadvantages**: 
  - Learning rate diminishes over time, leading to slow convergence.

- **When to Use**: 
  - Sparse data where features have different frequencies.

- **When Not to Use**: 
  - Avoid for long training runs due to diminishing learning rate.

---

## 4. RMSprop

- **Math**:  
  $$ 
  G_t = \beta G_{t-1} + (1 - \beta) \nabla_\theta^2 J(\theta) 
  $$
  $$ 
  \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta J(\theta) 
  $$

- **Intuition**: 
  - Maintains a moving average of squared gradients to stabilize learning.

- **Advantages**: 
  - Prevents rapid decay in learning rate.

- **Disadvantages**: 
  - May not perform as well on non-stationary data compared to Adam.

- **When to Use**: 
  - Common in training RNNs and LSTMs.

- **When Not to Use**: 
  - Not ideal if Adam is performing better.

---

## 5. Adam (Adaptive Moment Estimation)

- **Math**:  
  $$ 
  m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta) 
  $$
  $$ 
  v_t = \beta_2 v_{t-1} + (1 - \beta_2) \nabla_\theta^2 J(\theta) 
  $$
  $$ 
  \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t 
  $$

- **Intuition**: 
  - Combines momentum and adaptive learning rates for robust optimization.

- **Advantages**: 
  - Fast convergence, handles non-stationary problems well.

- **Disadvantages**: 
  - Can converge to suboptimal solutions if not fine-tuned.

- **When to Use**: 
  - Ideal for most deep learning applications.

- **When Not to Use**: 
  - If Adam converges too quickly, consider switching to SGD for fine-tuning.

---

## Summary Table of Optimizers

| Optimizer  | Problem                           | Solution                               | When to Use                           | When Not to Use                      |
|------------|-----------------------------------|----------------------------------------|---------------------------------------|--------------------------------------|
| **SGD**    | Gets stuck in local minima        | Use **Momentum** or adaptive optimizers| Suitable for convex functions         | Avoid for non-convex functions      |
| **Momentum**| High oscillations/overshooting    | Use **Learning Rate Decay** or Adam   | Deep networks                         | When dealing with noisy gradients    |
| **Adagrad**| Stops learning (no convergence)   | Use **RMSprop** or **Adam**           | Sparse data                          | Avoid for long training runs         |
| **RMSprop**| Non-optimal for some tasks        | Try **Adam** or **Nadam**             | Training RNNs and LSTMs              | Not ideal if Adam is better          |
| **Adam**   | Converges to suboptimal solutions | Fine-tune with **SGD**                 | Most deep learning applications       | If converging too quickly            |

---

- **SGD**: Shows a noisy and irregular path, frequently getting stuck in local minima.
- **Momentum**: Demonstrates smoother but oscillatory movement, overshooting the minima.
- **Adagrad**: Starts strong but quickly decays the learning rate, leading to stagnation.
- **RMSprop**: Maintains a consistent path with adaptive steps, avoiding rapid decay.
- **Adam**: Exhibits a balanced path with adaptive momentum, steadily converging to the minimum.

This visualization helps to understand the behavior and trade-offs of each optimizer as they navigate the loss landscape, showcasing how different strategies influence convergence and optimization in deep learning.
