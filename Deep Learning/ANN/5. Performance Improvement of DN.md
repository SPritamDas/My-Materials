## Hyperparameter Optimization in Deep Learning

### Key Hyperparameters
- **Hidden Layer**: Increase till overfitting.
- **Neurons/Layer**: Sufficient, use pyramid structure.
- **Activation Function**: Adjust based on problem (ReLU, sigmoid, etc.).
- **Optimizer**: Select appropriate optimizer (e.g., Adam, SGD).
- **Epochs**: Ensure slow but steady training. Use callbacks.
- **Batch Size**: Start with 8192, LR scheduler for 8-32.
- **Learning Rate**: Small value, adjust using a learning rate scheduler.
  
### Common Problems in Deep Learning Models
- **Vanishing Gradient**: Address with ReLU, proper weight initialization, batch normalization, and residual networks.
- **Exploding Gradient**: Apply gradient clipping to handle this.
- **Slow Training**: Use learning rate schedulers and optimizers.
- **Overfitting**: Implement L1/L2 regularization, dropout.
- **Not Enough Data**: Use transfer learning and unsupervised pre-training.
